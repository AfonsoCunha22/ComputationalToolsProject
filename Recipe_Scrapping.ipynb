{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritux\\AppData\\Local\\Temp\\ipykernel_13132\\2114608133.py:3: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "#Creating a clean Slate\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf')\n",
    "#Directing to the correct location and getting the file\n",
    "import os\n",
    "os.chdir('C:/Users/ritux/OneDrive - Danmarks Tekniske Universitet/Skrivebord/3 -Computational Tools for Data Science/')\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Referer': 'https://www.google.com/'  # Sometimes needed to bypass blocks\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_scrapping(base_url,limit,headers=headers):\n",
    "\n",
    "    urls = [] # List of urls\n",
    "\n",
    "    to_visit = [base_url] # Queue\n",
    "    while to_visit and len(urls) < limit:\n",
    "        \n",
    "        # Getting an URL form the queue\n",
    "        current_url = to_visit.pop(0)\n",
    "        # Issuing the request for access to the webpage\n",
    "        response = requests.get(current_url, headers=headers)\n",
    "        # Ensuring we are successful in the request\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Request failed with status code {response.status_code} at url: {current_url}\")\n",
    "        \n",
    "        # Obtaining the information in the webpage\n",
    "        Soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Iterating through the linked pages from the page we are visiting\n",
    "        for a in Soup.find_all('a', href=True): \n",
    "            url = urljoin(base_url, a['href'])\n",
    "            # Ensuring it stays within the chosen page\n",
    "            if url.startswith(base_url) and url: \n",
    "                # Keeping only new urls \n",
    "                if url not in urls:\n",
    "                    urls.append(url)\n",
    "                    # Adding to queue only unscheduled urls\n",
    "                    if url not in to_visit: \n",
    "                        to_visit.append(url)\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_urls(urls, limit,condition,headers=headers):\n",
    "    filtered_urls = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Fetch the page\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Request failed with status code {response.status_code} at url: {url}\")\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Check if the page contains the ingredient selector\n",
    "            if soup.select(condition) and url not in filtered_urls:\n",
    "                filtered_urls.append(url)\n",
    "                \n",
    "                # Stop if we've reached the limit\n",
    "                if len(filtered_urls) >= limit:\n",
    "                    break\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            continue  # Skip to the next URL in case of an error\n",
    "\n",
    "    return filtered_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter URLs\n",
    "def bcc_filter_urls(urls, limit,headers=headers):\n",
    "    filtered_urls = []\n",
    "    for url in urls:\n",
    "        segments = url.split('/')\n",
    "        if url.startswith('https://www.bbc.co.uk/') and url.startswith('https://www.bbc.co.uk/food/recipes') and len(segments) in [6,7]:\n",
    "            try:\n",
    "                response = requests.get(url,headers=headers)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Request failed with status code {response.status_code} at url: {url}\")\n",
    "            \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Check if the page contains the ingredients\n",
    "                if soup.select_one(\"div.recipe-ingredients-wrapper ul.recipe-ingredients__list li.recipe-ingredients__list-item a.recipe-ingredients__link\")  and url not in filtered_urls:\n",
    "                    filtered_urls.append(url)\n",
    "                    if len(filtered_urls) >= limit:\n",
    "                        break\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                continue  # Skip to the next URL in case of an error\n",
    "\n",
    "    return filtered_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ingredients from BBC website\n",
    "def bbc_get_ingredients(urls):\n",
    "    ingredients = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.select_one(\"h1.content-title__text\").text.strip()\n",
    "        recipe_ingredients = []\n",
    "        # Use soup.select with the CSS selector for ingredients\n",
    "        for item in soup.select(\"div.recipe-ingredients-wrapper ul.recipe-ingredients__list li.recipe-ingredients__list-item a.recipe-ingredients__link\"):\n",
    "            name = item.text.strip()\n",
    "            # Combine to form a full ingredient line\n",
    "            recipe_ingredients.append({'title': title,'description': name})\n",
    "        ingredients.append(recipe_ingredients)\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_get_ingredients(urls):\n",
    "    ingredients = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.select_one(\"title\").text.strip()\n",
    "        recipe_ingredients = []\n",
    "        # Use soup.select with the CSS selector for ingredients\n",
    "        for item in soup.select(\"ul.mm-recipes-structured-ingredients__list > li.mm-recipes-structured-ingredients__list-item\"):\n",
    "            name = item.select_one(\"span[data-ingredient-name]\").get_text(strip=True)\n",
    "            # Combine to form a full ingredient line\n",
    "            recipe_ingredients.append({'title': title,'description': name})\n",
    "        ingredients.append(recipe_ingredients)\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_get_ingredients(urls):\n",
    "    ingredients = []\n",
    "    for url in urls:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.select_one(\"title\").text.strip()\n",
    "        recipe_ingredients = []\n",
    "        # Use soup.select with the CSS selector for ingredients\n",
    "        for item in soup.select(\".wprm-recipe-ingredient\"):\n",
    "            # Get name if they exist\n",
    "            name = item.select_one(\".wprm-recipe-ingredient-name\")\n",
    "            if name:# Add to ingredients list\n",
    "                name = name.get_text(strip=True)\n",
    "                # Combine to form a full ingredient line\n",
    "                recipe_ingredients.append({'title': title,'description': name})\n",
    "                \n",
    "        ingredients.append(recipe_ingredients)\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 403 at url: https://www.twopeasandtheirpod.com/wp-login.php?redirect_to=https%3A%2F%2Fwww.twopeasandtheirpod.com%2Fsaved-recipes%2F\n"
     ]
    }
   ],
   "source": [
    "bbc_url = 'https://www.bbc.co.uk/food/recipes'\n",
    "all_url = \"https://www.allrecipes.com\"\n",
    "two_url = 'https://www.twopeasandtheirpod.com/'\n",
    "\n",
    "bbc_scraped_urls = url_scrapping(bbc_url,1000)\n",
    "all_scraped_urls = url_scrapping(all_url,1000)\n",
    "two_scraped_urls = url_scrapping(two_url,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/facebook/login?client_id=alrcom&tab_id=swrShh18Yq4&session_code=HWYwSygV2-iLyaAFvoDFm6hNI-_Ohbgz80ojxcQM_QQ\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/google/login?client_id=alrcom&tab_id=swrShh18Yq4&session_code=HWYwSygV2-iLyaAFvoDFm6hNI-_Ohbgz80ojxcQM_QQ\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/login-actions/registration?client_id=alrcom&tab_id=swrShh18Yq4\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/facebook/login?client_id=alrcom&tab_id=qE8HMqXMlvI&session_code=a9e2BsNbGyZyiir1CYlNxRFpNfM8mYg8cB4EvlmtLAc\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/google/login?client_id=alrcom&tab_id=qE8HMqXMlvI&session_code=a9e2BsNbGyZyiir1CYlNxRFpNfM8mYg8cB4EvlmtLAc\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/login-actions/registration?client_id=alrcom&tab_id=qE8HMqXMlvI\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/facebook/login?client_id=alrcom&tab_id=Nw2Nshedp5A&session_code=ncOJcCqpInQq_aH09ta15KCIs4eyKa0jHRkVlHNAWHs\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/google/login?client_id=alrcom&tab_id=Nw2Nshedp5A&session_code=ncOJcCqpInQq_aH09ta15KCIs4eyKa0jHRkVlHNAWHs\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/login-actions/registration?client_id=alrcom&tab_id=Nw2Nshedp5A\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/facebook/login?client_id=alrcom&tab_id=7-9mTf_PbV4&session_code=AqvwQXXQc56zJ4MrQyATgUCIRcKHMGob1LTfs3ZU-6I\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/google/login?client_id=alrcom&tab_id=7-9mTf_PbV4&session_code=AqvwQXXQc56zJ4MrQyATgUCIRcKHMGob1LTfs3ZU-6I\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/login-actions/registration?client_id=alrcom&tab_id=7-9mTf_PbV4\n",
      "Request failed with status code 403 at url: https://www.twopeasandtheirpod.com/wp-login.php?redirect_to=https%3A%2F%2Fwww.twopeasandtheirpod.com%2Fsaved-recipes%2F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritux\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 403 at url: https://www.twopeasandtheirpod.com/wp-login.php/\n"
     ]
    }
   ],
   "source": [
    "all_con = \"li.mm-recipes-structured-ingredients__list-item\"\n",
    "two_con = \".wprm-recipe-ingredient\"\n",
    "\n",
    "bbc_url_filterd = bcc_filter_urls(bbc_scraped_urls, 1000,headers=headers)\n",
    "all_url_filterd = filter_urls(all_scraped_urls, 1000,all_con,headers=headers)\n",
    "two_url_filterd = filter_urls(two_scraped_urls, 1000,two_con,headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ingredients = two_get_ingredients(two_url_filterd)\n",
    "all_ingredients = all_get_ingredients(all_url_filterd)\n",
    "bcc_ingredients = bbc_get_ingredients(bbc_url_filterd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the three websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For bbc:\n",
      "Number of URLs found : 1010\n",
      "Number of URLs filtered: 552\n",
      "Number of URLs with recipes: 552\n",
      "\n",
      "\n",
      "For all:\n",
      "Number of URLs found : 1311\n",
      "Number of URLs filtered: 575\n",
      "Number of URLs with recipes: 575\n",
      "\n",
      "\n",
      "For two:\n",
      "Number of URLs found : 1025\n",
      "Number of URLs filtered: 386\n",
      "Number of URLs with recipes: 386\n"
     ]
    }
   ],
   "source": [
    "print(\"For bbc:\")\n",
    "print(\"Number of URLs found :\" , len(bbc_scraped_urls))\n",
    "print(\"Number of URLs filtered:\" , len(bbc_url_filterd))\n",
    "print(\"Number of URLs with recipes:\" , len(bcc_ingredients))\n",
    "n_bcc =len(bcc_ingredients)\n",
    "print(\"\\n\")\n",
    "print(\"For all:\")\n",
    "print(\"Number of URLs found :\" , len(all_scraped_urls))\n",
    "print(\"Number of URLs filtered:\" , len(all_url_filterd))\n",
    "print(\"Number of URLs with recipes:\" , len(all_ingredients))\n",
    "n_all =len(all_ingredients)\n",
    "print(\"\\n\")\n",
    "print(\"For two:\")\n",
    "print(\"Number of URLs found :\" , len(two_scraped_urls))\n",
    "print(\"Number of URLs filtered:\" , len(two_url_filterd))\n",
    "print(\"Number of URLs with recipes:\" , len(two_ingredients))\n",
    "n_two =len(two_ingredients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count  = 0\n",
    "all_ingredients_df = pd.DataFrame(columns=['id','source','link', 'title'])\n",
    "\n",
    "# Extract ingredients\n",
    "for n in range(n_bcc):\n",
    "    count +=1\n",
    "    ingredients_df = pd.DataFrame(bcc_ingredients[n])\n",
    "    ingredients_df['id'] = count\n",
    "    ingredients_df['link'] = bbc_url_filterd[n]\n",
    "    ingredients_df['source'] = 'bbc'\n",
    "    all_ingredients_df = pd.concat([all_ingredients_df, ingredients_df], ignore_index=True)\n",
    "\n",
    "for n in range(n_all):\n",
    "    count +=1\n",
    "    ingredients_df = pd.DataFrame(all_ingredients[n])\n",
    "    ingredients_df['id'] = count\n",
    "    ingredients_df['link'] = all_url_filterd[n]\n",
    "    ingredients_df['source'] = 'all'\n",
    "    all_ingredients_df = pd.concat([all_ingredients_df, ingredients_df], ignore_index=True)\n",
    "\n",
    "for n in range(n_two):\n",
    "    count +=1\n",
    "    ingredients_df = pd.DataFrame(all_ingredients[n])\n",
    "    ingredients_df['id'] = count\n",
    "    ingredients_df['link'] = all_url_filterd[n]\n",
    "    ingredients_df['source'] = 'two'\n",
    "    all_ingredients_df = pd.concat([all_ingredients_df, ingredients_df], ignore_index=True)\n",
    "\n",
    "\n",
    "all_ingredients_df.to_csv('recipes_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/chicken_and...</td>\n",
       "      <td>Chicken and tomato pasta</td>\n",
       "      <td>olive oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/chicken_and...</td>\n",
       "      <td>Chicken and tomato pasta</td>\n",
       "      <td>chicken breasts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/chicken_and...</td>\n",
       "      <td>Chicken and tomato pasta</td>\n",
       "      <td>butter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/chicken_and...</td>\n",
       "      <td>Chicken and tomato pasta</td>\n",
       "      <td>garlic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/chicken_and...</td>\n",
       "      <td>Chicken and tomato pasta</td>\n",
       "      <td>rosemary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16721</th>\n",
       "      <td>1513</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.allrecipes.com/creamy-cajun-potato...</td>\n",
       "      <td>Creamy Cajun Potato Soup Recipe</td>\n",
       "      <td>andouille smoked sausage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16722</th>\n",
       "      <td>1513</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.allrecipes.com/creamy-cajun-potato...</td>\n",
       "      <td>Creamy Cajun Potato Soup Recipe</td>\n",
       "      <td>vegetable broth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16723</th>\n",
       "      <td>1513</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.allrecipes.com/creamy-cajun-potato...</td>\n",
       "      <td>Creamy Cajun Potato Soup Recipe</td>\n",
       "      <td>heavy cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16724</th>\n",
       "      <td>1513</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.allrecipes.com/creamy-cajun-potato...</td>\n",
       "      <td>Creamy Cajun Potato Soup Recipe</td>\n",
       "      <td>Cajun seasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16725</th>\n",
       "      <td>1513</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.allrecipes.com/creamy-cajun-potato...</td>\n",
       "      <td>Creamy Cajun Potato Soup Recipe</td>\n",
       "      <td>grated Parmesan cheese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16726 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id source                                               link  \\\n",
       "0         1    bbc  https://www.bbc.co.uk/food/recipes/chicken_and...   \n",
       "1         1    bbc  https://www.bbc.co.uk/food/recipes/chicken_and...   \n",
       "2         1    bbc  https://www.bbc.co.uk/food/recipes/chicken_and...   \n",
       "3         1    bbc  https://www.bbc.co.uk/food/recipes/chicken_and...   \n",
       "4         1    bbc  https://www.bbc.co.uk/food/recipes/chicken_and...   \n",
       "...     ...    ...                                                ...   \n",
       "16721  1513    two  https://www.allrecipes.com/creamy-cajun-potato...   \n",
       "16722  1513    two  https://www.allrecipes.com/creamy-cajun-potato...   \n",
       "16723  1513    two  https://www.allrecipes.com/creamy-cajun-potato...   \n",
       "16724  1513    two  https://www.allrecipes.com/creamy-cajun-potato...   \n",
       "16725  1513    two  https://www.allrecipes.com/creamy-cajun-potato...   \n",
       "\n",
       "                                 title               description  \n",
       "0             Chicken and tomato pasta                 olive oil  \n",
       "1             Chicken and tomato pasta           chicken breasts  \n",
       "2             Chicken and tomato pasta                    butter  \n",
       "3             Chicken and tomato pasta                    garlic  \n",
       "4             Chicken and tomato pasta                  rosemary  \n",
       "...                                ...                       ...  \n",
       "16721  Creamy Cajun Potato Soup Recipe  andouille smoked sausage  \n",
       "16722  Creamy Cajun Potato Soup Recipe           vegetable broth  \n",
       "16723  Creamy Cajun Potato Soup Recipe               heavy cream  \n",
       "16724  Creamy Cajun Potato Soup Recipe           Cajun seasoning  \n",
       "16725  Creamy Cajun Potato Soup Recipe    grated Parmesan cheese  \n",
       "\n",
       "[16726 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ingredients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_urls = bbc_scraped_urls + all_scraped_urls + two_scraped_urls\n",
    "scraped_urls_df = pd.DataFrame({'scraped_urls': scraped_urls})\n",
    "scraped_urls_df.to_csv('scraped_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_filterd = bbc_url_filterd + all_url_filterd + two_url_filterd\n",
    "url_filterd_df = pd.DataFrame({'url_filterd': url_filterd})\n",
    "url_filterd_df.to_csv('url_filterd.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ingredients_df.to_csv('recipes_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
