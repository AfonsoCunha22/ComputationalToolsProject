{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritux\\AppData\\Local\\Temp\\ipykernel_3476\\2114608133.py:3: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "#Creating a clean Slate\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf')\n",
    "#Directing to the correct location and getting the file\n",
    "import os\n",
    "os.chdir('C:/Users/ritux/OneDrive - Danmarks Tekniske Universitet/Skrivebord/3 -Computational Tools for Data Science/')\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Referer': 'https://www.google.com/'  # Sometimes needed to bypass blocks\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_scrapping(base_url,limit,headers=headers):\n",
    "\n",
    "    urls = [] # List of urls\n",
    "\n",
    "    to_visit = [base_url] # Queue\n",
    "    while to_visit and len(urls) < limit:\n",
    "        \n",
    "        # Getting an URL form the queue\n",
    "        current_url = to_visit.pop(0)\n",
    "        # Issuing the request for access to the webpage\n",
    "        response = requests.get(current_url, headers=headers)\n",
    "        # Ensuring we are successful in the request\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Request failed with status code {response.status_code} at url: {current_url}\")\n",
    "        \n",
    "        # Obtaining the information in the webpage\n",
    "        Soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Iterating through the linked pages from the page we are visiting\n",
    "        for a in Soup.find_all('a', href=True): \n",
    "            url = urljoin(base_url, a['href'])\n",
    "            # Ensuring it stays within the chosen page\n",
    "            if url.startswith(base_url) and url: \n",
    "                # Keeping only new urls \n",
    "                if url not in urls:\n",
    "                    urls.append(url)\n",
    "                    # Adding to queue only unscheduled urls\n",
    "                    if url not in to_visit: \n",
    "                        to_visit.append(url)\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_urls(urls, limit,condition,headers=headers):\n",
    "    filtered_urls = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Fetch the page\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Request failed with status code {response.status_code} at url: {url}\")\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Check if the page contains the ingredient selector\n",
    "            if soup.select(condition) and url not in filtered_urls:\n",
    "                filtered_urls.append(url)\n",
    "                \n",
    "                # Stop if we've reached the limit\n",
    "                if len(filtered_urls) >= limit:\n",
    "                    break\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            continue  # Skip to the next URL in case of an error\n",
    "\n",
    "    return filtered_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter URLs\n",
    "def bcc_filter_urls(urls, limit,headers=headers):\n",
    "    filtered_urls = []\n",
    "    for url in urls:\n",
    "        segments = url.split('/')\n",
    "        if url.startswith('https://www.bbc.co.uk/') and url.startswith('https://www.bbc.co.uk/food/recipes') and len(segments) in [6,7]:\n",
    "            try:\n",
    "                response = requests.get(url,headers=headers)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Request failed with status code {response.status_code} at url: {url}\")\n",
    "            \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Check if the page contains the ingredients\n",
    "                if soup.select_one(\"div.recipe-ingredients-wrapper ul.recipe-ingredients__list li.recipe-ingredients__list-item a.recipe-ingredients__link\")  and url not in filtered_urls:\n",
    "                    filtered_urls.append(url)\n",
    "                    if len(filtered_urls) >= limit:\n",
    "                        break\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                continue  # Skip to the next URL in case of an error\n",
    "\n",
    "    return filtered_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 403 at url: https://www.twopeasandtheirpod.com/wp-login.php?redirect_to=https%3A%2F%2Fwww.twopeasandtheirpod.com%2Fsaved-recipes%2F\n"
     ]
    }
   ],
   "source": [
    "bbc_url = 'https://www.bbc.co.uk/food/recipes'\n",
    "all_url = \"https://www.allrecipes.com\"\n",
    "two_url = 'https://www.twopeasandtheirpod.com/'\n",
    "\n",
    "bbc_scraped_urls = url_scrapping(bbc_url,1000)\n",
    "all_scraped_urls = url_scrapping(all_url,1000)\n",
    "two_scraped_urls = url_scrapping(two_url,1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/facebook/login?client_id=alrcom&tab_id=PCb85YRPuiA&session_code=L-GTFLh2QNkiFK66QX2LT8xX4UGZzc0AVN8XcnS1AKs\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/google/login?client_id=alrcom&tab_id=PCb85YRPuiA&session_code=L-GTFLh2QNkiFK66QX2LT8xX4UGZzc0AVN8XcnS1AKs\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/login-actions/registration?client_id=alrcom&tab_id=PCb85YRPuiA\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/facebook/login?client_id=alrcom&tab_id=cKMwMrHZXlg&session_code=gZmGCS6p3-j7d5hlVy6BV3gDVs4QwQkCKlISvrjD7gM\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/google/login?client_id=alrcom&tab_id=cKMwMrHZXlg&session_code=gZmGCS6p3-j7d5hlVy6BV3gDVs4QwQkCKlISvrjD7gM\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/login-actions/registration?client_id=alrcom&tab_id=cKMwMrHZXlg\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/facebook/login?client_id=alrcom&tab_id=NbehRTCmQKk&session_code=xKuBEhPcKMa1GVkh8PGTUU9rKUBCQxy586sm8QxrU4A\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/google/login?client_id=alrcom&tab_id=NbehRTCmQKk&session_code=xKuBEhPcKMa1GVkh8PGTUU9rKUBCQxy586sm8QxrU4A\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/login-actions/registration?client_id=alrcom&tab_id=NbehRTCmQKk\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/facebook/login?client_id=alrcom&tab_id=oeQZXFl2dMI&session_code=yD5EHrn8_DTNl5BifIuupBBwraMyY1epJgT908jYze4\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/broker/google/login?client_id=alrcom&tab_id=oeQZXFl2dMI&session_code=yD5EHrn8_DTNl5BifIuupBBwraMyY1epJgT908jYze4\n",
      "Request failed with status code 404 at url: https://www.allrecipes.com/realms/alrcom/login-actions/registration?client_id=alrcom&tab_id=oeQZXFl2dMI\n",
      "Request failed with status code 403 at url: https://www.twopeasandtheirpod.com/wp-login.php?redirect_to=https%3A%2F%2Fwww.twopeasandtheirpod.com%2Fsaved-recipes%2F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritux\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code 403 at url: https://www.twopeasandtheirpod.com/wp-login.php/\n"
     ]
    }
   ],
   "source": [
    "all_con = \"li.mm-recipes-structured-ingredients__list-item\"\n",
    "two_con = \".wprm-recipe-ingredient\"\n",
    "\n",
    "bbc_url_filterd = bcc_filter_urls(bbc_scraped_urls, 1000,headers=headers)\n",
    "all_url_filterd = filter_urls(all_scraped_urls, 1000,all_con,headers=headers)\n",
    "two_url_filterd = filter_urls(two_scraped_urls, 1000,two_con,headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_get_ingredients(urls):\n",
    "    ingredients = []\n",
    "    for url in urls:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        recipe_ingredients = []\n",
    "        # Use soup.select with the CSS selector for ingredients\n",
    "        for item in soup.select(\".wprm-recipe-ingredient\"):\n",
    "            # Get name if they exist\n",
    "            name = item.select_one(\".wprm-recipe-ingredient-name\")\n",
    "            if name:# Add to ingredients list\n",
    "                recipe_ingredients.append(name.get_text(strip=True))\n",
    "                \n",
    "        ingredients.append(recipe_ingredients)\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_ingredients = two_get_ingredients(two_url_filterd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_get_ingredients(urls):\n",
    "    ingredients = []\n",
    "    for url in urls:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        recipe_ingredients = []\n",
    "        # Use soup.select with the CSS selector for ingredients\n",
    "        for item in soup.select(\"ul.mm-recipes-structured-ingredients__list > li.mm-recipes-structured-ingredients__list-item\"):\n",
    "            # Extract the ingredient\n",
    "            name = item.select_one(\"span[data-ingredient-name]\").get_text(strip=True)\n",
    "            # Combine to form a full ingredient line\n",
    "            recipe_ingredients.append(name)\n",
    "        ingredients.append(recipe_ingredients)\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ingredients = all_get_ingredients(all_url_filterd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ingredients from BBC website\n",
    "def extract_ingredients_bbc(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve webpage. Status code:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    ingredient_names = []\n",
    "    \n",
    "    title = soup.select_one(\"h1.content-title__text\").text.strip()\n",
    "    \n",
    "    ingredients_elements = soup.select(\"div.recipe-ingredients-wrapper ul.recipe-ingredients__list li.recipe-ingredients__list-item a.recipe-ingredients__link\")  # Use CSS selector for the ingredients span elements\n",
    "        \n",
    "    for ingredient_element in ingredients_elements:\n",
    "        ingredient_names.append(ingredient_element.text.strip())\n",
    "            \n",
    "\n",
    "    return ingredient_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract ingredients from BBC website\n",
    "def bcc_get_ingredients(urls):\n",
    "    ingredients = []\n",
    "    for url in bbc_url_filterd:\n",
    "        ingredients.append(extract_ingredients_bbc(url))\n",
    "    return ingredients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcc_ingredients = bcc_get_ingredients(bbc_url_filterd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the three websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For bbc:\n",
      "Number of URLs found : 1013\n",
      "Number of URLs filtered: 552\n",
      "Number of URLs with recipes: 552\n",
      "\n",
      "\n",
      "For all:\n",
      "Number of URLs found : 1311\n",
      "Number of URLs filtered: 581\n",
      "Number of URLs with recipes: 581\n",
      "\n",
      "\n",
      "For two:\n",
      "Number of URLs found : 1085\n",
      "Number of URLs filtered: 417\n",
      "Number of URLs with recipes: 417\n"
     ]
    }
   ],
   "source": [
    "print(\"For bbc:\")\n",
    "print(\"Number of URLs found :\" , len(bbc_scraped_urls))\n",
    "print(\"Number of URLs filtered:\" , len(bbc_url_filterd))\n",
    "print(\"Number of URLs with recipes:\" , len(bcc_ingredients))\n",
    "n_bcc =len(bcc_ingredients)\n",
    "print(\"\\n\")\n",
    "print(\"For all:\")\n",
    "print(\"Number of URLs found :\" , len(all_scraped_urls))\n",
    "print(\"Number of URLs filtered:\" , len(all_url_filterd))\n",
    "print(\"Number of URLs with recipes:\" , len(all_ingredients))\n",
    "n_all =len(all_ingredients)\n",
    "print(\"\\n\")\n",
    "print(\"For two:\")\n",
    "print(\"Number of URLs found :\" , len(two_scraped_urls))\n",
    "print(\"Number of URLs filtered:\" , len(two_url_filterd))\n",
    "print(\"Number of URLs with recipes:\" , len(two_ingredients))\n",
    "n_two =len(two_ingredients)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count  = 0\n",
    "all_ingredients_df = pd.DataFrame(columns=['id','source','link', 'title'])\n",
    "for n in range(n_bcc):\n",
    "    count += 1\n",
    "    ingredients_df= pd.DataFrame(bcc_ingredients[n])\n",
    "    ingredients_df['id'] = count\n",
    "    ingredients_df['source'] = 'bbc'\n",
    "    ingredients_df['link'] = bbc_url_filterd[n]\n",
    "    ingredients_df['title'] = ' '    \n",
    "    all_ingredients_df = pd.concat([all_ingredients_df, ingredients_df], ignore_index=True)\n",
    "\n",
    "for n in range(n_all):\n",
    "    count += 1\n",
    "    ingredients_df= pd.DataFrame(all_ingredients[n])\n",
    "    ingredients_df['id'] = count\n",
    "    ingredients_df['source'] = 'all'\n",
    "    ingredients_df['link'] = all_url_filterd[n]\n",
    "    ingredients_df['title'] = ' '\n",
    "    all_ingredients_df = pd.concat([all_ingredients_df, ingredients_df], ignore_index=True)\n",
    "\n",
    "for n in range(n_two):\n",
    "    count += 1\n",
    "    ingredients_df= pd.DataFrame(two_ingredients[n])\n",
    "    ingredients_df['id'] = count\n",
    "    ingredients_df['source'] = 'two'\n",
    "    ingredients_df['link'] = two_url_filterd[n]\n",
    "    ingredients_df['title'] = ' '\n",
    "    all_ingredients_df = pd.concat([all_ingredients_df, ingredients_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/healthy_chi...</td>\n",
       "      <td></td>\n",
       "      <td>beef mince</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/healthy_chi...</td>\n",
       "      <td></td>\n",
       "      <td>red onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/healthy_chi...</td>\n",
       "      <td></td>\n",
       "      <td>garlic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/healthy_chi...</td>\n",
       "      <td></td>\n",
       "      <td>courgette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc</td>\n",
       "      <td>https://www.bbc.co.uk/food/recipes/healthy_chi...</td>\n",
       "      <td></td>\n",
       "      <td>aubergine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18668</th>\n",
       "      <td>1550</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.twopeasandtheirpod.com/pumpkin-bun...</td>\n",
       "      <td></td>\n",
       "      <td>100% pure pumpkin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18669</th>\n",
       "      <td>1550</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.twopeasandtheirpod.com/pumpkin-bun...</td>\n",
       "      <td></td>\n",
       "      <td>cream cheese,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18670</th>\n",
       "      <td>1550</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.twopeasandtheirpod.com/pumpkin-bun...</td>\n",
       "      <td></td>\n",
       "      <td>confectioner's sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18671</th>\n",
       "      <td>1550</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.twopeasandtheirpod.com/pumpkin-bun...</td>\n",
       "      <td></td>\n",
       "      <td>vanilla extract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18672</th>\n",
       "      <td>1550</td>\n",
       "      <td>two</td>\n",
       "      <td>https://www.twopeasandtheirpod.com/pumpkin-bun...</td>\n",
       "      <td></td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18673 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id source                                               link title  \\\n",
       "0         1    bbc  https://www.bbc.co.uk/food/recipes/healthy_chi...         \n",
       "1         1    bbc  https://www.bbc.co.uk/food/recipes/healthy_chi...         \n",
       "2         1    bbc  https://www.bbc.co.uk/food/recipes/healthy_chi...         \n",
       "3         1    bbc  https://www.bbc.co.uk/food/recipes/healthy_chi...         \n",
       "4         1    bbc  https://www.bbc.co.uk/food/recipes/healthy_chi...         \n",
       "...     ...    ...                                                ...   ...   \n",
       "18668  1550    two  https://www.twopeasandtheirpod.com/pumpkin-bun...         \n",
       "18669  1550    two  https://www.twopeasandtheirpod.com/pumpkin-bun...         \n",
       "18670  1550    two  https://www.twopeasandtheirpod.com/pumpkin-bun...         \n",
       "18671  1550    two  https://www.twopeasandtheirpod.com/pumpkin-bun...         \n",
       "18672  1550    two  https://www.twopeasandtheirpod.com/pumpkin-bun...         \n",
       "\n",
       "                          0  \n",
       "0                beef mince  \n",
       "1                 red onion  \n",
       "2                    garlic  \n",
       "3                 courgette  \n",
       "4                 aubergine  \n",
       "...                     ...  \n",
       "18668     100% pure pumpkin  \n",
       "18669         cream cheese,  \n",
       "18670  confectioner's sugar  \n",
       "18671       vanilla extract  \n",
       "18672                  milk  \n",
       "\n",
       "[18673 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ingredients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ingredients_df.to_csv('recipes_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_urls = bbc_scraped_urls + all_scraped_urls + two_scraped_urls\n",
    "scraped_urls_df = pd.DataFrame({'scraped_urls': scraped_urls})\n",
    "scraped_urls_df.to_csv('scraped_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_filterd = bbc_url_filterd + all_url_filterd + two_url_filterd\n",
    "url_filterd_df = pd.DataFrame({'url_filterd': url_filterd})\n",
    "url_filterd_df.to_csv('url_filterd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('recipes_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 12.0\n",
      "Minimum: 1\n",
      "Maximum: 44\n",
      "Standard deviation: 5.382200654329305\n"
     ]
    }
   ],
   "source": [
    "# Median number of ingredients per id\n",
    "median_ingredients = df.groupby('id').size().median()\n",
    "print(f\"Average: {median_ingredients}\")\n",
    "\n",
    "# Minimum number of ingredients per id\n",
    "min_ingredients = df.groupby('id').size().min()\n",
    "print(f\"Minimum: {min_ingredients}\")\n",
    "\n",
    "# Maximum number of ingredients per id\n",
    "max_ingredients = df.groupby('id').size().max()\n",
    "print(f\"Maximum: {max_ingredients}\")\n",
    "\n",
    "# Standard deviation of the number of ingredients per id\n",
    "std_ingredients = df.groupby('id').size().std()\n",
    "print(f\"Standard deviation: {std_ingredients}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 40594\n",
      "Unique words: 2193\n",
      "Most common words: [('salt', 1065), ('oil', 901), ('pepper', 878), ('ground', 837), ('sugar', 806), ('black', 723), ('butter', 569), ('flour', 569), ('chopped', 516), ('olive', 494), ('garlic', 488), ('chicken', 455), ('powder', 405), ('vanilla', 395), ('cream', 394)]\n",
      "Most common stop words: [('or', 502), ('and', 475), ('to', 347), ('of', 192), ('into', 139), ('as', 116), ('for', 92), ('more', 82), ('can', 37), ('with', 33), ('in', 25), ('if', 8), ('at', 6), ('such', 6), ('on', 6)]\n",
      "Total amount of stop words: 2102\n"
     ]
    }
   ],
   "source": [
    "words = [word for document in df['0'] for word in str(document).split()]\n",
    "\n",
    "# Word Count\n",
    "total_words = len(words)\n",
    "print(f'Total words: {total_words}')\n",
    "\n",
    "# Unique Word Count\n",
    "unique_words = len(set(words))\n",
    "print(f'Unique words: {unique_words}')\n",
    "\n",
    "\n",
    "\n",
    "# Most Common Words (excluding stopwords)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "non_stopwords = [word for word in words if word not in stop_words]\n",
    "most_common_words = Counter(non_stopwords).most_common(15)\n",
    "print(f'Most common words: {most_common_words}')\n",
    "\n",
    "# Most Common Stop Words\n",
    "stopwords_in_text = [word for word in words if word in stop_words]\n",
    "most_common_stopwords = Counter(stopwords_in_text).most_common(15)\n",
    "print(f'Most common stop words: {most_common_stopwords}')\n",
    "#count of stop words in df\n",
    "print(f\"Total amount of stop words: {len(stopwords_in_text)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
